import torch
import torch.nn as nn
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, roc_curve, auc
import shap
import os
from tqdm import tqdm
from utils.dataset import CSVDataset
from utils.model import LSTMClassifier
from torch.utils.data import DataLoader

# è®¾å¤‡é…ç½®
DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f'ä½¿ç”¨è®¾å¤‡: {DEVICE}')

def load_model_and_data():
    """åŠ è½½æ¨¡å‹å’Œæµ‹è¯•æ•°æ®"""
    print('ğŸ“‚ æ­£åœ¨åŠ è½½æ¨¡å‹å’Œæµ‹è¯•æ•°æ®...')
    
    # åŠ è½½æµ‹è¯•æ•°æ®é›†
    test_dataset = CSVDataset(root_dir='data', train=False)
    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)
    
    # ä»æ•°æ®é›†è·å–è¾“å…¥ç»´åº¦
    input_size = test_dataset.input_size
    sequence_length = test_dataset.sequence_length
    
    print(f'æµ‹è¯•é›†æ ·æœ¬æ•°: {len(test_dataset)}')
    print(f'è¾“å…¥ç»´åº¦: {input_size}, åºåˆ—é•¿åº¦: {sequence_length}')
    
    # åŠ è½½æ¨¡å‹
    model = LSTMClassifier(input_size=input_size, hidden_size=256, num_layers=1)
    model_path = 'ckpt/best_model.pth'
    model.load_state_dict(torch.load(model_path, map_location=DEVICE))
    model = model.to(DEVICE)
    model.eval()
    
    print('âœ… æ¨¡å‹å’Œæ•°æ®åŠ è½½æˆåŠŸ!')
    return model, test_dataset, test_loader, input_size, sequence_length

def evaluate_model(model, test_loader):
    """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
    print('\nğŸ“Š å¼€å§‹æ¨¡å‹è¯„ä¼°...')
    
    all_predictions = []
    all_labels = []
    all_probs = []
    
    with torch.no_grad():
        for inputs, labels in tqdm(test_loader, desc='è¯„ä¼°ä¸­'):
            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)
            outputs = model(inputs)
            probs = outputs.squeeze().cpu().numpy()
            predictions = (outputs > 0.5).float().squeeze().cpu().numpy()
            
            all_probs.extend(probs)
            all_predictions.extend(predictions)
            all_labels.extend(labels.cpu().numpy())
    
    all_labels = np.array(all_labels)
    all_predictions = np.array(all_predictions)
    all_probs = np.array(all_probs)
    
    # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
    accuracy = np.mean(all_predictions == all_labels)
    tp = np.sum((all_predictions == 1) & (all_labels == 1))
    fp = np.sum((all_predictions == 1) & (all_labels == 0))
    fn = np.sum((all_predictions == 0) & (all_labels == 1))
    tn = np.sum((all_predictions == 0) & (all_labels == 0))
    
    precision = tp / (tp + fp) if (tp + fp) > 0 else 0
    recall = tp / (tp + fn) if (tp + fn) > 0 else 0
    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
    
    # ä¿å­˜è¯„ä¼°ç»“æœ
    metrics = {
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1_score': f1,
        'true_positives': tp,
        'false_positives': fp,
        'false_negatives': fn,
        'true_negatives': tn
    }
    
    with open('result/evaluation_metrics.txt', 'w') as f:
        f.write('æ¨¡å‹è¯„ä¼°æŒ‡æ ‡:\n')
        f.write(f'å‡†ç¡®ç‡: {accuracy:.4f}\n')
        f.write(f'ç²¾ç¡®ç‡: {precision:.4f}\n')
        f.write(f'å¬å›ç‡: {recall:.4f}\n')
        f.write(f'F1åˆ†æ•°: {f1:.4f}\n')
        f.write(f'\næ··æ·†çŸ©é˜µ:\n')
        f.write(f'TP: {tp}, FP: {fp}\n')
        f.write(f'FN: {fn}, TN: {tn}\n')
    
    print(f'\nâœ… è¯„ä¼°å®Œæˆ!')
    print(f'å‡†ç¡®ç‡: {accuracy:.4f}')
    print(f'ç²¾ç¡®ç‡: {precision:.4f}')
    print(f'å¬å›ç‡: {recall:.4f}')
    print(f'F1åˆ†æ•°: {f1:.4f}')
    
    return all_labels, all_predictions, all_probs, metrics

def plot_confusion_matrix(y_true, y_pred):
    """ç»˜åˆ¶æ··æ·†çŸ©é˜µå¹¶ä¿å­˜åŸå§‹æ•°æ®"""
    print('ğŸ“ˆ æ­£åœ¨ç»˜åˆ¶æ··æ·†çŸ©é˜µ...')
    
    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=['No Risk (0)', 'Risk (1)'],
                yticklabels=['No Risk (0)', 'Risk (1)'])
    plt.title('Confusion Matrix')
    plt.xlabel('Predicted Label')
    plt.ylabel('True Label')
    plt.tight_layout()
    plt.savefig('result/figures/confusion_matrix.png', dpi=300, bbox_inches='tight')
    plt.close()
    
    # ä¿å­˜åŸå§‹æ•°æ®
    cm_df = pd.DataFrame(cm, 
                        index=['True No Risk', 'True Risk'],
                        columns=['Pred No Risk', 'Pred Risk'])
    cm_df.to_csv('result/raw_data/confusion_matrix_data.csv')
    print('âœ… æ··æ·†çŸ©é˜µæ•°æ®å·²ä¿å­˜åˆ° result/raw_data/confusion_matrix_data.csv')
    
    print('âœ… æ··æ·†çŸ©é˜µå·²ä¿å­˜åˆ° result/figures/confusion_matrix.png')

def plot_roc_curve(y_true, y_probs):
    """ç»˜åˆ¶ROCæ›²çº¿å¹¶ä¿å­˜åŸå§‹æ•°æ®"""
    print('ğŸ“ˆ æ­£åœ¨ç»˜åˆ¶ROCæ›²çº¿...')
    
    fpr, tpr, thresholds = roc_curve(y_true, y_probs)
    roc_auc = auc(fpr, tpr)
    
    plt.figure(figsize=(8, 6))
    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC Curve (AUC = {roc_auc:.4f})')
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate (FPR)')
    plt.ylabel('True Positive Rate (TPR)')
    plt.title('ROC Curve')
    plt.legend(loc="lower right")
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig('result/figures/roc_curve.png', dpi=300, bbox_inches='tight')
    plt.close()
    
    # ä¿å­˜åŸå§‹æ•°æ®
    roc_data = pd.DataFrame({
        'fpr': fpr,
        'tpr': tpr,
        'thresholds': thresholds
    })
    roc_data.to_csv('result/raw_data/roc_curve_data.csv', index=False)
    with open('result/raw_data/roc_auc_value.txt', 'w') as f:
        f.write(f'AUC: {roc_auc:.4f}')
    
    print(f'âœ… ROCæ›²çº¿æ•°æ®å·²ä¿å­˜ï¼ŒAUC = {roc_auc:.4f}')

    # ä¿å­˜AUCå€¼
    with open('result/evaluation_metrics.txt', 'a') as f:
        f.write(f'\nAUC: {roc_auc:.4f}\n')

def plot_prediction_distribution(y_probs, y_true):
    """ç»˜åˆ¶é¢„æµ‹æ¦‚ç‡åˆ†å¸ƒå¹¶ä¿å­˜åŸå§‹æ•°æ®"""
    print('ğŸ“ˆ æ­£åœ¨ç»˜åˆ¶é¢„æµ‹æ¦‚ç‡åˆ†å¸ƒ...')
    
    plt.figure(figsize=(10, 6))
    sns.histplot(data=pd.DataFrame({'Probability': y_probs, 'True Label': y_true.astype(str)}),
                 x='Probability', hue='True Label', bins=50, kde=True, alpha=0.6)
    plt.axvline(x=0.5, color='r', linestyle='--', label='Decision Threshold')
    plt.title('Prediction Probability Distribution')
    plt.xlabel('Probability of being predicted as Risk')
    plt.ylabel('Number of Samples')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig('result/figures/prediction_distribution.png', dpi=300, bbox_inches='tight')
    plt.close()
    
    # ä¿å­˜åŸå§‹æ•°æ®
    dist_data = pd.DataFrame({
        'probability': y_probs,
        'true_label': y_true
    })
    dist_data.to_csv('result/raw_data/prediction_distribution_data.csv', index=False)
    print('âœ… é¢„æµ‹æ¦‚ç‡åˆ†å¸ƒæ•°æ®å·²ä¿å­˜åˆ° result/raw_data/prediction_distribution_data.csv')
    
    print('âœ… é¢„æµ‹æ¦‚ç‡åˆ†å¸ƒå›¾å·²ä¿å­˜')

def explain_with_shap(model, test_dataset, input_size, sequence_length):
    """ä½¿ç”¨SHAPè§£é‡Šæ¨¡å‹"""
    print('\nğŸ” å¼€å§‹SHAPè§£é‡Š...')
    
    # å‡†å¤‡èƒŒæ™¯æ•°æ®ï¼ˆä½¿ç”¨éƒ¨åˆ†æµ‹è¯•æ•°æ®ä½œä¸ºèƒŒæ™¯ï¼‰
    background_size = min(100, len(test_dataset))
    background_indices = np.random.choice(len(test_dataset), background_size, replace=False)
    background_data = []
    
    for idx in background_indices:
        data, _ = test_dataset[idx]
        background_data.append(data.numpy())
    
    background = torch.tensor(np.array(background_data), dtype=torch.float32).to(DEVICE)
    print(f'ä½¿ç”¨ {background_size} ä¸ªæ ·æœ¬ä½œä¸ºSHAPèƒŒæ™¯æ•°æ®')
    
    # åˆ›å»ºSHAPè§£é‡Šå™¨
    def model_forward(x):
        """æ¨¡å‹å‰å‘ä¼ æ’­å‡½æ•°ï¼Œé€‚é…SHAP"""
        # xæ˜¯numpyæ•°ç»„ï¼Œéœ€è¦è½¬æ¢ä¸ºPyTorchå¼ é‡
        x_tensor = torch.tensor(x, dtype=torch.float32)
        batch_size = x_tensor.shape[0]
        x_tensor = x_tensor.reshape(batch_size, input_size, sequence_length)
        x_tensor = x_tensor.to(DEVICE)
        with torch.no_grad():
            outputs = model(x_tensor)
        return outputs.cpu().numpy()
    
    # åˆå§‹åŒ–SHAPè§£é‡Šå™¨
    explainer = shap.KernelExplainer(model_forward, background.reshape(background_size, -1).cpu().numpy())
    
    # é€‰æ‹©è¦è§£é‡Šçš„æ ·æœ¬ï¼ˆæ¯ä¸ªç±»åˆ«é€‰æ‹©å‡ ä¸ªä»£è¡¨æ€§æ ·æœ¬ï¼‰
    sample_indices = []
    labels = [test_dataset[i][1].item() for i in range(len(test_dataset))]
    labels = np.array(labels)
    
    # ä»æ¯ä¸ªç±»åˆ«ä¸­é€‰æ‹©5ä¸ªæ ·æœ¬
    for label in [0, 1]:
        indices = np.where(labels == label)[0]
        if len(indices) > 5:
            sample_indices.extend(np.random.choice(indices, 5, replace=False))
        else:
            sample_indices.extend(indices)
    
    print(f'å°†è§£é‡Š {len(sample_indices)} ä¸ªæ ·æœ¬çš„é¢„æµ‹')
    
    # è·å–SHAPå€¼
    test_samples = []
    test_labels = []
    for idx in sample_indices:
        data, label = test_dataset[idx]
        test_samples.append(data.numpy())
        test_labels.append(label.item())
    
    test_samples = np.array(test_samples)
    test_labels = np.array(test_labels)
    
    # è®¡ç®—SHAPå€¼ - å¯¹äºäºŒåˆ†ç±»ï¼ŒKernelExplainerå¯èƒ½è¿”å›ä¸¤ä¸ªæ•°ç»„çš„åˆ—è¡¨
    shap_values_all = explainer.shap_values(test_samples.reshape(len(test_samples), -1), nsamples=100)
    # æˆ‘ä»¬éœ€è¦æ­£ç±»ï¼ˆç±»åˆ«1ï¼‰çš„SHAPå€¼
    if isinstance(shap_values_all, list) and len(shap_values_all) > 1:
        shap_values = shap_values_all[1]
    elif isinstance(shap_values_all, list) and len(shap_values_all) == 1:
        shap_values = shap_values_all[0]
    else:
        shap_values = shap_values_all
    
    # ä¿å­˜SHAPåˆ†æç»“æœ
    np.save('result/shap/shap_values.npy', shap_values)
    np.save('result/shap/test_samples.npy', test_samples)
    np.save('result/shap/test_labels.npy', test_labels)
    
    # ä¿å­˜SHAPåŸå§‹æ•°æ®
    shap_df = pd.DataFrame(shap_values)
    shap_df.to_csv('result/raw_data/shap_values_raw.csv', index=False)
    print('âœ… SHAPåŸå§‹å€¼å·²ä¿å­˜åˆ° result/raw_data/shap_values_raw.csv')
    
    print('âœ… SHAPå€¼è®¡ç®—å®Œæˆï¼Œç»“æœå·²ä¿å­˜')
    
    # ç»˜åˆ¶SHAPæ‘˜è¦å›¾
    plot_shap_summary(shap_values, test_samples, test_labels, input_size, sequence_length)
    
    # ç»˜åˆ¶å•ä¸ªæ ·æœ¬çš„SHAPåŠ›å›¾
    plot_shap_force_plots(shap_values, test_samples, test_labels, sample_indices[:2])  # åªå±•ç¤ºå‰2ä¸ªæ ·æœ¬
    
    return shap_values, test_samples, test_labels

def plot_shap_summary(shap_values, samples, labels, input_size, sequence_length):
    """ç»˜åˆ¶SHAPæ‘˜è¦å›¾å¹¶ä¿å­˜åŸå§‹æ•°æ®"""
    print('ğŸ“ˆ æ­£åœ¨ç»˜åˆ¶SHAPæ‘˜è¦å›¾...')
    
    # å°†æ•°æ®é‡å¡‘ä¸º2Dæ ¼å¼ç”¨äºSHAPå¯è§†åŒ–
    feature_names = []
    for i in range(input_size):
        for t in range(sequence_length):
            feature_names.append(f'Feature_{i}_Time_Step_{t}')
    
    # åˆ›å»ºæ‘˜è¦å›¾
    plt.figure(figsize=(12, 8))
    shap.summary_plot(shap_values, samples.reshape(len(samples), -1), 
                     feature_names=feature_names, show=False)
    plt.title('SHAP Value Summary Plot', fontsize=14)
    plt.tight_layout()
    plt.savefig('result/figures/shap_summary.png', dpi=300, bbox_inches='tight')
    plt.close()
    
    # ä¿å­˜åŸå§‹SHAPæ•°æ®
    shap_data = pd.DataFrame(shap_values, columns=feature_names)
    shap_data['true_label'] = labels
    shap_data.to_csv('result/raw_data/shap_summary_data.csv', index=False)
    
    # ä¿å­˜æ ·æœ¬æ•°æ®
    samples_reshaped = samples.reshape(len(samples), -1)
    samples_df = pd.DataFrame(samples_reshaped, columns=feature_names)
    samples_df['true_label'] = labels
    samples_df.to_csv('result/raw_data/shap_samples_data.csv', index=False)
    
    print('âœ… SHAPæ‘˜è¦æ•°æ®å·²ä¿å­˜åˆ° result/raw_data/shap_summary_data.csv')
    print('âœ… SHAPæ ·æœ¬æ•°æ®å·²ä¿å­˜åˆ° result/raw_data/shap_samples_data.csv')
    print('âœ… SHAPæ‘˜è¦å›¾å·²ä¿å­˜åˆ° result/figures/shap_summary.png')

def plot_shap_force_plots(shap_values, samples, labels, sample_indices):
    """ç»˜åˆ¶å•ä¸ªæ ·æœ¬çš„SHAPåŠ›å›¾å¹¶ä¿å­˜åŸå§‹æ•°æ®"""
    print('ğŸ“ˆ ç»˜åˆ¶SHAPåŠ›å›¾...')
    
    for i, idx in enumerate(sample_indices):
        if i >= len(shap_values):
            print(f"âš ï¸ è­¦å‘Š: ç´¢å¼• {i} è¶…å‡ºSHAPå€¼èŒƒå›´ï¼Œè·³è¿‡æ­¤æ ·æœ¬")
            continue
            
        sample_features = samples[i].flatten()
        sample_shap_values = shap_values[i]
        
        # if len(sample_features) != len(sample_shap_values):
        #     print(f"âš ï¸ è­¦å‘Š: æ ·æœ¬ {idx} çš„ç‰¹å¾é•¿åº¦ ({len(sample_features)}) å’Œ SHAPå€¼é•¿åº¦ ({len(sample_shap_values)}) ä¸åŒ¹é…ï¼Œè·³è¿‡æ­¤æ ·æœ¬")
        #     continue
        
        plt.figure(figsize=(12, 3))
        shap_values_single = sample_shap_values.reshape(1, -1)
        features_single = sample_features.reshape(1, -1)
        
        shap.force_plot(
            base_value=0.5,
            shap_values=shap_values_single,
            features=features_single,
            matplotlib=True,
            show=False
        )
        
        plt.title(f'SHAP Force Plot for Sample {idx} (True Label: {labels[i]})', fontsize=12)
        plt.tight_layout()
        plt.savefig(f'result/figures/shap_force_sample_{idx}.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        # ä¿å­˜å•ä¸ªæ ·æœ¬çš„SHAPæ•°æ®
        force_data = pd.DataFrame({
            'feature_value': features_single.flatten(),
            'shap_value': shap_values_single.flatten()
        })
        feature_names = [f'Feature_{j//8}_Time_{j%8}' for j in range(len(features_single.flatten()))]
        force_data['feature_name'] = feature_names
        force_data.to_csv(f'result/raw_data/shap_force_sample_{idx}_data.csv', index=False)
        print(f'âœ… æ ·æœ¬ {idx} çš„SHAPåŠ›å›¾æ•°æ®å·²ä¿å­˜åˆ° result/raw_data/shap_force_sample_{idx}_data.csv')
    
    print(f'âœ… SHAPåŠ›å›¾å·²ä¿å­˜ï¼Œå…± {len(sample_indices)} ä¸ªæ ·æœ¬')

def plot_feature_importance(shap_values, samples, input_size, sequence_length):
    """ç»˜åˆ¶ç‰¹å¾é‡è¦æ€§å¹¶ä¿å­˜åŸå§‹æ•°æ®"""
    print('ğŸ“ˆ æ­£åœ¨ç»˜åˆ¶ç‰¹å¾é‡è¦æ€§...')
    
    # è®¡ç®—æ¯ä¸ªç‰¹å¾ç»´åº¦çš„å¹³å‡SHAPç»å¯¹å€¼
    shap_abs = np.abs(shap_values)
    feature_importance = np.zeros(input_size)
    
    for i in range(input_size):
        # æå–è¯¥ç‰¹å¾åœ¨æ‰€æœ‰æ—¶é—´æ­¥çš„SHAPå€¼
        feature_shap = shap_abs[:, i*sequence_length:(i+1)*sequence_length]
        feature_importance[i] = np.mean(feature_shap)
    
    # åˆ›å»ºç‰¹å¾é‡è¦æ€§å›¾
    feature_names = ['Attendance', 'Participation', 'Homework Completion', 'Homework Quality', 
                    'Quiz Performance', 'Interaction', 'Study Time', 'Phone Usage']
    
    plt.figure(figsize=(12, 6))
    sns.barplot(x=feature_importance, y=feature_names, palette='viridis')
    plt.title('Feature Importance (based on SHAP values)', fontsize=14)
    plt.xlabel('Mean |SHAP Value|', fontsize=12)
    plt.tight_layout()
    plt.savefig('result/figures/feature_importance.png', dpi=300, bbox_inches='tight')
    plt.close()
    
    # ä¿å­˜ç‰¹å¾é‡è¦æ€§æ•°æ®
    importance_df = pd.DataFrame({
        'Feature': feature_names,
        'Importance': feature_importance
    })
    importance_df = importance_df.sort_values('Importance', ascending=False)
    importance_df.to_csv('result/feature_importance.csv', index=False)
    importance_df.to_csv('result/raw_data/feature_importance_data.csv', index=False)  # åŒæ—¶ä¿å­˜åˆ°raw_data
    
    print('âœ… ç‰¹å¾é‡è¦æ€§æ•°æ®å·²ä¿å­˜åˆ° result/raw_data/feature_importance_data.csv')
    print('âœ… ç‰¹å¾é‡è¦æ€§å›¾å·²ä¿å­˜ï¼Œæ•°æ®å·²ä¿å­˜åˆ° result/feature_importance.csv')

def generate_report(metrics, shap_analysis=False):
    """ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š"""
    print('\nğŸ“„ æ­£åœ¨ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š...')
    
    report = f"""
# æ¨¡å‹è¯„ä¼°æŠ¥å‘Š

## åŸºæœ¬ä¿¡æ¯
- è¯„ä¼°æ—¶é—´: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}
- è®¾å¤‡: {DEVICE}
- æµ‹è¯•é›†æ ·æœ¬æ•°é‡: {metrics.get('test_size', 0)}

## æ€§èƒ½æŒ‡æ ‡
- **å‡†ç¡®ç‡**: {metrics['accuracy']:.4f}
- **ç²¾ç¡®ç‡**: {metrics['precision']:.4f}
- **å¬å›ç‡**: {metrics['recall']:.4f}
- **F1åˆ†æ•°**: {metrics['f1_score']:.4f}
- **AUC**: {metrics.get('auc', 0):.4f}

## æ··æ·†çŸ©é˜µ
- çœŸé˜³æ€§ (TP): {metrics['true_positives']}
- å‡é˜³æ€§ (FP): {metrics['false_positives']}
- å‡é˜´æ€§ (FN): {metrics['false_negatives']}
- çœŸé˜´æ€§ (TN): {metrics['true_negatives']}

## ç»“æœå¯è§†åŒ–
- æ··æ·†çŸ©é˜µ: ![Confusion Matrix](figures/confusion_matrix.png)
- ROCæ›²çº¿: ![ROC Curve](figures/roc_curve.png)
- é¢„æµ‹æ¦‚ç‡åˆ†å¸ƒ: ![Prediction Distribution](figures/prediction_distribution.png)
"""

    if shap_analysis:
        report += """
## SHAPåˆ†æ
- SHAPæ‘˜è¦å›¾: ![SHAP Summary](figures/shap_summary.png)
- ç‰¹å¾é‡è¦æ€§: ![Feature Importance](figures/feature_importance.png)
- SHAPåŠ›å›¾ç¤ºä¾‹: ![SHAP Force Plot](figures/shap_force_sample_586.png)
- SHAPå€¼æ•°æ®: `result/shap/`
- ç‰¹å¾é‡è¦æ€§æ•°æ®: `result/feature_importance.csv`
- åŸå§‹æ•°æ®: `result/raw_data/`
""" 
    report += """
## ç»“è®º
æ¨¡å‹åœ¨å­¦ä¸šé£é™©é¢„æµ‹ä»»åŠ¡ä¸Šè¡¨ç°è‰¯å¥½ã€‚é‡ç‚¹å…³æ³¨ä»¥ä¸‹æ–¹é¢ï¼š
1. æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆè¯†åˆ«æœ‰å­¦ä¸šé£é™©çš„å­¦ç”Ÿ
2. ä¸»è¦å½±å“å› ç´ åŒ…æ‹¬ï¼š{top_features}
3. å»ºè®®å¯¹é«˜é£é™©å­¦ç”Ÿè¿›è¡Œæ—©æœŸå¹²é¢„
"""
    
    # æ·»åŠ ç‰¹å¾é‡è¦æ€§æ€»ç»“
    if os.path.exists('result/feature_importance.csv'):
        importance_df = pd.read_csv('result/feature_importance.csv')
        top_features = ', '.join(importance_df['Feature'].head(3).tolist())
        report = report.replace('{top_features}', top_features)
    else:
        report = report.replace('{top_features}', 'Attendance, Quiz Performance, Homework Completion')
    
    with open('result/evaluation_report.md', 'w', encoding='utf-8') as f:
        f.write(report)
    
    print('âœ… æœ€ç»ˆæŠ¥å‘Šå·²ä¿å­˜åˆ° result/evaluation_report.md')

if __name__ == '__main__':
    # è®¾ç½®éšæœºç§å­
    torch.manual_seed(42)
    np.random.seed(42)
    
    print('ğŸ¯ å¼€å§‹æµ‹è¯•é˜¶æ®µ...')
    
    try:
        # åŠ è½½æ¨¡å‹å’Œæ•°æ®
        model, test_dataset, test_loader, input_size, sequence_length = load_model_and_data()
        
        # è¯„ä¼°æ¨¡å‹
        y_true, y_pred, y_probs, metrics = evaluate_model(model, test_loader)
        metrics['test_size'] = len(test_dataset)
        
        # ç»˜åˆ¶è¯„ä¼°å›¾è¡¨
        plot_confusion_matrix(y_true, y_pred)
        plot_roc_curve(y_true, y_probs)
        plot_prediction_distribution(y_probs, y_true)
        
        # SHAPè§£é‡Š
        shap_values, samples, labels = explain_with_shap(model, test_dataset, input_size, sequence_length)
        plot_feature_importance(shap_values, samples, input_size, sequence_length)
        
        # ç”ŸæˆæŠ¥å‘Š
        generate_report(metrics, shap_analysis=True)
        
        print('\nğŸ‰ æµ‹è¯•å®Œæˆï¼æ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ° result/ ç›®å½•')
        print('ğŸ“ ç»“æœç›®å½•ç»“æ„:')
        print('result/')
        print('â”œâ”€â”€ evaluation_metrics.txt')
        print('â”œâ”€â”€ evaluation_report.md')
        print('â”œâ”€â”€ feature_importance.csv')
        print('â”œâ”€â”€ figures/')
        print('â”‚   â”œâ”€â”€ confusion_matrix.png')
        print('â”‚   â”œâ”€â”€ roc_curve.png')
        print('â”‚   â”œâ”€â”€ prediction_distribution.png')
        print('â”‚   â”œâ”€â”€ shap_summary.png')
        print('â”‚   â”œâ”€â”€ feature_importance.png')
        print('â”‚   â””â”€â”€ shap_force_sample_*.png')
        print('â”œâ”€â”€ shap/')
        print('â”‚   â”œâ”€â”€ shap_values.npy')
        print('â”‚   â”œâ”€â”€ test_samples.npy')
        print('â”‚   â””â”€â”€ test_labels.npy')
        print('â””â”€â”€ raw_data/')
        print('    â”œâ”€â”€ confusion_matrix_data.csv')
        print('    â”œâ”€â”€ roc_curve_data.csv')
        print('    â”œâ”€â”€ roc_auc_value.txt')
        print('    â”œâ”€â”€ prediction_distribution_data.csv')
        print('    â”œâ”€â”€ shap_summary_data.csv')
        print('    â”œâ”€â”€ shap_samples_data.csv')
        print('    â”œâ”€â”€ shap_values_raw.csv')
        print('    â”œâ”€â”€ shap_force_sample_*_data.csv')
        print('    â””â”€â”€ feature_importance_data.csv')
        
    except Exception as e:
        print(f'âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}')
        import traceback
        traceback.print_exc()